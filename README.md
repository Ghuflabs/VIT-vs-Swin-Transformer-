# VIT-vs-Swin-Transformer
This project involves fine-tuning Vision Transformer (ViT) and Swin Transformer models on the CIFAR-10 dataset and visualizing their attention mechanisms using Grad-CAM to interpret how each model makes decisions. After training for 50 epochs, the ViT model achieved a final training loss of 0.373 and a validation accuracy of 67.40%, while the Swin Transformer reached a final training loss of 0.366 with a slightly higher validation accuracy of 67.73%. These results indicate both models were able to learn meaningful representations from CIFAR-10, with Swin Transformer performing marginally better. Grad-CAM was applied to both models to visualize the regions of the image that contributed most to the predictions. For ViT, Grad-CAM produced smooth, centralized heatmaps, typically focusing on the most discriminative parts of the objectâ€”reflecting its global self-attention mechanism. In contrast, Swin Transformer Grad-CAM visualizations varied based on the layer and transformation used. When Grad-CAM was applied using the norm1 layer without spatial reshaping, the heatmaps appeared noisy or striped due to the lack of spatial alignment in token outputs. However, using the attn layer along with a custom reshape_transform function, which re-maps the sequential output into a spatial grid, resulted in much clearer and semantically meaningful attention maps, with multiple localized focus points. This reflects Swin Transformer's hierarchical, window-based self-attention structure, which emphasizes localized feature processing. The comparison between ViT and Swin Transformer highlights how architectural differences influence both quantitative performance and qualitative interpretability, emphasizing the importance of correct spatial reshaping when visualizing attention in transformer-based models.
